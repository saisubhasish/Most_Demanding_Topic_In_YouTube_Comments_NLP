{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import nltk   # nltk is for natural language processing and computational linguistics\n",
        "from nltk.corpus import stopwords   # corpus is a collection of authentic text or audio organized into datasets\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer    # To analyse sentiment\n",
        "from sklearn.feature_extraction.text import CountVectorizer    # method to convert text to numerical data\n",
        "from sklearn.decomposition import LatentDirichletAllocation    # explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar"
      ],
      "metadata": {
        "id": "iOpldXm_eGnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "QAHBh1z8d2yo",
        "outputId": "5bc44fb9-c3d0-4b66-ec7a-5437663f9b14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0                                                  0\n",
              "0            0  And, could you also put up some material on ho...\n",
              "1            1  HI sir ,thanks for sharing your knowledge it r...\n",
              "2            2  i love how you spelling it, if we could build ...\n",
              "3            3  I just love how you keep reassuring us in the ...\n",
              "4            4  At 13:00 you had mentioned about weakness of L...\n",
              "5            5  Can you please guide me on learning NLP with R...\n",
              "6            6  Sir, will we be able to do text analytics and ...\n",
              "7            7  I think anyone who is in nlp has eventually fo...\n",
              "8            8  i come across with all these concepts in my or...\n",
              "9            9  Great Sir, Big fan of yours.  Thanks a lot for...\n",
              "10          10  Hello Sir I have been following you and have c...\n",
              "11          11  Have you ever used spacy when it comes to NLP?...\n",
              "12          12  Can you please do the video on unstructured te...\n",
              "13          13  He is that teacher which our education system ...\n",
              "14          14  If I need to cover NLP, is this playlist suffi...\n",
              "15          15  It  was really helpful. Can u make  videos on ...\n",
              "16          16                               This is really a gem\n",
              "17          17              Thanks for this wonderful NLP content\n",
              "18          18  I just can't thank you enough, words would not...\n",
              "19          19  Sir I am following this playlist but I am stuc..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21e405a7-18eb-4460-bcc9-570c32d5b818\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>And, could you also put up some material on ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>HI sir ,thanks for sharing your knowledge it r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>i love how you spelling it, if we could build ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I just love how you keep reassuring us in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>At 13:00 you had mentioned about weakness of L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>Can you please guide me on learning NLP with R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>Sir, will we be able to do text analytics and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>I think anyone who is in nlp has eventually fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>i come across with all these concepts in my or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>Great Sir, Big fan of yours.  Thanks a lot for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>Hello Sir I have been following you and have c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>Have you ever used spacy when it comes to NLP?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>Can you please do the video on unstructured te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>He is that teacher which our education system ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>If I need to cover NLP, is this playlist suffi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>It  was really helpful. Can u make  videos on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>This is really a gem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>Thanks for this wonderful NLP content</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>I just can't thank you enough, words would not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>Sir I am following this playlist but I am stuc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21e405a7-18eb-4460-bcc9-570c32d5b818')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-21e405a7-18eb-4460-bcc9-570c32d5b818 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-21e405a7-18eb-4460-bcc9-570c32d5b818');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "path = '/content/youtube_comments_scrapped.csv'\n",
        "\n",
        "# Reading data\n",
        "df = pd.read_csv(path)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')     # used to eliminate unimportant words (commonly used words)\n",
        "nltk.download('punkt')    # a tokenizer that divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.\n",
        "nltk.download('vader_lexicon')    # is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWthLJYkehz2",
        "outputId": "ab64455d-ce29-4645-be53-e61460365c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the comments o remove the unwanted words\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "AU7OltYQeFak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Frequency Analysis\n",
        "\n",
        "def word_frequency_analysis(comments):\n",
        "    \"\"\"\n",
        "    This function prints frequency of each word in a descending order\n",
        "    \"\"\"\n",
        "    # Convert comments to strings and handle float values\n",
        "    comments = [str(comment) if not pd.isnull(comment) else '' for comment in comments]\n",
        "\n",
        "    # Combine all comments into a single string\n",
        "    all_comments = ' '.join(comments)\n",
        "\n",
        "    # Tokenize the comments\n",
        "    tokens = nltk.word_tokenize(all_comments)\n",
        "\n",
        "    # Filter out stopwords\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    word_freq = nltk.FreqDist(filtered_tokens)\n",
        "\n",
        "    # Print the most common words\n",
        "    print('Most common words:')\n",
        "    for word, freq in word_freq.most_common(10):\n",
        "        print(f'{word}: {freq}')"
      ],
      "metadata": {
        "id": "MdkEpQSkeXqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "def sentiment_analysis(comments):\n",
        "    \"\"\"\n",
        "    This function prints the over all sentiment of the text\n",
        "    \"\"\"\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Calculate sentiment scores for each comment\n",
        "    sentiment_scores = [sid.polarity_scores(comment) for comment in comments]\n",
        "\n",
        "    # Calculate average sentiment scores\n",
        "    avg_sentiment = sum(score['compound'] for score in sentiment_scores) / len(sentiment_scores)\n",
        "\n",
        "    print(f'Average sentiment: {avg_sentiment}')"
      ],
      "metadata": {
        "id": "E-I6fezgelwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic Modeling\n",
        "\n",
        "def topic_modeling(comments):\n",
        "    \"\"\"\n",
        "    This function prints the most discussed topics in the comments\n",
        "    \"\"\"\n",
        "    # Create a CountVectorizer object\n",
        "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "    \n",
        "    # Fit and transform the comments\n",
        "    tf = tf_vectorizer.fit_transform(comments)\n",
        "    \n",
        "    # Create an LDA model\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    \n",
        "    # Fit the LDA model\n",
        "    lda.fit(tf)\n",
        "    \n",
        "    # Print the top words for each topic\n",
        "    print('Top words per topic:')\n",
        "    feature_names = tf_vectorizer.get_feature_names_out()\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
        "        print(f\"Topic {topic_idx+1}: {' '.join(top_words)}\")\n"
      ],
      "metadata": {
        "id": "FzvhWnvRen1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns of the data\n",
        "\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v6MKlXae8HG",
        "outputId": "36e80555-2f63-4c9c-f74d-f70b5a640561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', '0'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform analysis on the comments\n",
        "\n",
        "comments = df['0'].tolist()"
      ],
      "metadata": {
        "id": "lX8hB06IepyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of data\n",
        "\n",
        "comments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfvVlKRWe2KI",
        "outputId": "5f71ed6e-a7a9-4fd5-9b65-0071a1f65e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"And, could you also put up some material on how Hidden Markov Models are used in NLP? have studied them way back in 2011 during my Master's degree in the pre Deep Learning era. But don't have much practical exposure to NLP? And does acoustic model for phonemes recognition come more under speech Recognition? Could you also provide a short description on that?\",\n",
              " 'HI sir ,thanks for sharing your knowledge it really helps me alot sometime, i have a question. \\nif LSTM has problem , why cant we directly use bidirectional LSTM instead of LSTM , can we skip LSTM and directly apply Bidirectional LSTM ?',\n",
              " 'i love how you spelling it, if we could build such a pyramid for other techs, life of the learners would be much easier as it s enough to keep in mind this intuitive/organic scale of complexity.',\n",
              " 'I just love how you keep reassuring us in the video that you got us covered from bottom to top. This is super helpful. Thank you',\n",
              " 'At 13:00 you had mentioned about weakness of LSTM. If we take the use case of Statistical Machine Translation and I have 2 sentences in my training set :\\n1) \"I cross the river bank to reach primary school\"\\n2) \"I need to go to the bank to urgently withdraw funds\"\\nBoth are longish sentences. And if after conversion to Vector Representation, the word \"Bank\" has a different meaning in sentence 1) where we need to look at previous word river, while in sentence 2) we need to look ahead (right context).\\nIs this the issue with plain LSTM that Bidirectional LSTM is able to overcome?',\n",
              " 'Can you please guide me on learning NLP with R, I guess most of the libraries mentioned are python based.  I am not sure if these libraries can also be used in R language as well - Please advise. Thanks',\n",
              " 'Sir, will we be able to do text analytics and sentimental analysis after watching this playlist?',\n",
              " 'I think anyone who is in nlp has eventually followed this kind of roadmap even though the order may be a bit different.',\n",
              " 'i come across with all these concepts in my organisation but its hard me to working on question and answering using tensorflow  or pytorch but i am aware of rasa core n rasa nlu where it is easy to generate question and answers. krish why dont you make a video on question and answering using tensorflow',\n",
              " 'Great Sir, Big fan of yours.  Thanks a lot for detailed course.  Hats off.',\n",
              " 'Hello Sir I have been following you and have currently completed your Machine Learning playlist. So can you tell me that how much prior knowledge we need to have to start with this course, do we need deep learning, if it is so then please tell me do I need advance deep learning like Boltzmann Machine, Autoencoders, GANs and etc or is it okay to start now.',\n",
              " 'Have you ever used spacy when it comes to NLP? If yes: Would be lovely if you could cover how you used it...Thanks for your work',\n",
              " 'Can you please do the video on unstructured text to ontology using NLP. Thanks',\n",
              " \"He is that teacher which our education system genuinely needs . He's self taught hence that is visible in his lectures. Absolutely amazing !!\",\n",
              " 'If I need to cover NLP, is this playlist suffix or I need to refer to deep learning playlist also?',\n",
              " 'It  was really helpful. Can u make  videos on Grammer Correction using  Rule based methord, Language Models &  classifiers.\\nits really hard to understand it otherwise',\n",
              " 'This is really a gem',\n",
              " 'Thanks for this wonderful NLP content',\n",
              " \"I just can't thank you enough, words would not be able to compensate for your greatness !\",\n",
              " 'Sir I am following this playlist but I am stuck in between because I am not familier with ML,DL so I want to know the important topics of ML,DL which are using in NLP so I will learn only those topics otherwise the whole syllabus of ML,DL will takes too much time , please tell me sir what you prefer to me.?']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Frequency Analysis with count\n",
        "\n",
        "word_frequency_analysis(comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq-NozEGerwm",
        "outputId": "a6795029-56e0-4bfd-a172-997406803da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words:\n",
            ",: 23\n",
            ".: 23\n",
            "?: 10\n",
            "NLP: 8\n",
            "LSTM: 8\n",
            "need: 8\n",
            "): 5\n",
            "Thanks: 5\n",
            "playlist: 5\n",
            "using: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "sentiment_analysis(comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-MSmJkjetok",
        "outputId": "47a016ff-911c-4fc9-abb7-3def9f7081cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sentiment: 0.442875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic Modeling\n",
        "\n",
        "topic_modeling(comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZfFFdCEi9Mn",
        "outputId": "1d4b360f-2ae1-49a3-c675-b0e9547bcc62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words per topic:\n",
            "Topic 1: thank just love able video helpful text thanks cover using\n",
            "Topic 2: really using helpful video cover make hard language based models\n",
            "Topic 3: need playlist sir learning tell deep machine course following knowledge\n",
            "Topic 4: lstm question bidirectional need using use able sir different mentioned\n",
            "Topic 5: nlp thanks used learning deep come mentioned models language based\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edPVAkaOjWW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}