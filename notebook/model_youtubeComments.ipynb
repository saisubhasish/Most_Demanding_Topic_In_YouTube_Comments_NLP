{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "\n",
        "import nltk   # nltk is for natural language processing and computational linguistics\n",
        "from nltk.corpus import stopwords   # corpus is a collection of authentic text or audio organized into datasets\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer    # To analyse sentiment\n",
        "from sklearn.feature_extraction.text import CountVectorizer    # method to convert text to numerical data\n",
        "from sklearn.decomposition import LatentDirichletAllocation    # explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar"
      ],
      "metadata": {
        "id": "iOpldXm_eGnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "\n",
        "MONGO_DB_URL = \"mongodb+srv://MongoDB:mongodb123@cluster0.i7o85x8.mongodb.net/?retryWrites=true&w=majority\"\n",
        "mongo_client = pymongo.MongoClient(MONGO_DB_URL)\n",
        "\n",
        "def get_collection_as_dataframe(database_name:str,collection_name:str)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Description: This function return collection as dataframe\n",
        "    =========================================================\n",
        "    Params:\n",
        "    database_name: database name\n",
        "    collection_name: collection name\n",
        "    =========================================================\n",
        "    return Pandas dataframe of a collection\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Reading data from database\n",
        "        df = pd.DataFrame(list(mongo_client[database_name][collection_name].find()))\n",
        "        if \"_id\" in df.columns:\n",
        "            df = df.drop(\"_id\",axis=1)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        raise e"
      ],
      "metadata": {
        "id": "gwR1CUyOMnDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAHBh1z8d2yo"
      },
      "outputs": [],
      "source": [
        "DATABASE_NAME = 'NLP'\n",
        "COLLECTION_NAME = 'youtubeComments'\n",
        "\n",
        "# Reading data\n",
        "df = get_collection_as_dataframe(database_name= DATABASE_NAME, collection_name= COLLECTION_NAME)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')     # used to eliminate unimportant words (commonly used words)\n",
        "nltk.download('punkt')    # a tokenizer that divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.\n",
        "nltk.download('vader_lexicon')    # is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains."
      ],
      "metadata": {
        "id": "pWthLJYkehz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the comments o remove the unwanted words\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "AU7OltYQeFak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Frequency Analysis\n",
        "\n",
        "def word_frequency_analysis(comments):\n",
        "    \"\"\"\n",
        "    Description: This function prints frequency of each word in a descending order\n",
        "    =========================================================\n",
        "    Params:\n",
        "    comments: comments\n",
        "    =========================================================\n",
        "    returns most frequent top 10 words\n",
        "    \"\"\"\n",
        "    # Convert comments to strings and handle float values\n",
        "    comments = [str(comment) if not pd.isnull(comment) else '' for comment in comments]\n",
        "\n",
        "    # Combine all comments into a single string\n",
        "    all_comments = ' '.join(comments)\n",
        "\n",
        "    # Tokenize the comments\n",
        "    tokens = nltk.word_tokenize(all_comments)\n",
        "\n",
        "    # Filter out stopwords\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    word_freq = nltk.FreqDist(filtered_tokens)\n",
        "\n",
        "    # Print the most common words\n",
        "    print('Most common words:')\n",
        "    for word, freq in word_freq.most_common(10):\n",
        "        print(f'{word}: {freq}')"
      ],
      "metadata": {
        "id": "MdkEpQSkeXqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "def sentiment_analysis(comments):\n",
        "    \"\"\"\n",
        "    This function prints the over all sentiment of the text\n",
        "    \"\"\"\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Calculate sentiment scores for each comment\n",
        "    sentiment_scores = [sid.polarity_scores(comment) for comment in comments]\n",
        "\n",
        "    # Calculate average sentiment scores\n",
        "    avg_sentiment = sum(score['compound'] for score in sentiment_scores) / len(sentiment_scores)\n",
        "\n",
        "    print(f'Average sentiment: {avg_sentiment}')"
      ],
      "metadata": {
        "id": "E-I6fezgelwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic Modeling\n",
        "\n",
        "def topic_modeling(comments):\n",
        "    \"\"\"\n",
        "    Description: This function prints the most discussed topics in the comments\n",
        "    =========================================================\n",
        "    Params:\n",
        "    comments: comments\n",
        "    =========================================================\n",
        "    returns top 5 demanding topic\n",
        "    \"\"\"\n",
        "    # Create a CountVectorizer object\n",
        "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "\n",
        "    # Fit and transform the comments\n",
        "    tf = tf_vectorizer.fit_transform(comments)\n",
        "\n",
        "    # Create an LDA model\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "\n",
        "    # Fit the LDA model\n",
        "    lda.fit(tf)\n",
        "\n",
        "    # Print the top words for each topic\n",
        "    print('Top words per topic:')\n",
        "    feature_names = tf_vectorizer.get_feature_names_out()\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
        "        print(f\"Topic {topic_idx+1}: {' '.join(top_words)}\")\n"
      ],
      "metadata": {
        "id": "FzvhWnvRen1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns of the data\n",
        "\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v6MKlXae8HG",
        "outputId": "fdae8a96-cce8-409b-e149-87e4b62996a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', '0'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform analysis on the comments\n",
        "\n",
        "comments = df['0'].tolist()"
      ],
      "metadata": {
        "id": "lX8hB06IepyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of data\n",
        "\n",
        "comments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfvVlKRWe2KI",
        "outputId": "9d45b535-5909-411e-a3f1-15b101dc567d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I just love how you keep reassuring us in the video that you got us covered from bottom to top. This is super helpful. Thank you',\n",
              " \"He is that teacher which our education system genuinely needs . He's self taught hence that is visible in his lectures. Absolutely amazing !!\",\n",
              " \"And, could you also put up some material on how Hidden Markov Models are used in NLP? have studied them way back in 2011 during my Master's degree in the pre Deep Learning era. But don't have much practical exposure to NLP? And does acoustic model for phonemes recognition come more under speech Recognition? Could you also provide a short description on that?\",\n",
              " 'HI sir ,thanks for sharing your knowledge it really helps me alot sometime, i have a question. \\nif LSTM has problem , why cant we directly use bidirectional LSTM instead of LSTM , can we skip LSTM and directly apply Bidirectional LSTM ?',\n",
              " 'i love how you spelling it, if we could build such a pyramid for other techs, life of the learners would be much easier as it s enough to keep in mind this intuitive/organic scale of complexity.',\n",
              " \"I just can't thank you enough, words would not be able to compensate for your greatness !\",\n",
              " 'At 13:00 you had mentioned about weakness of LSTM. If we take the use case of Statistical Machine Translation and I have 2 sentences in my training set :\\n1) \"I cross the river bank to reach primary school\"\\n2) \"I need to go to the bank to urgently withdraw funds\"\\nBoth are longish sentences. And if after conversion to Vector Representation, the word \"Bank\" has a different meaning in sentence 1) where we need to look at previous word river, while in sentence 2) we need to look ahead (right context).\\nIs this the issue with plain LSTM that Bidirectional LSTM is able to overcome?',\n",
              " 'Have you ever used spacy when it comes to NLP? If yes: Would be lovely if you could cover how you used it...Thanks for your work',\n",
              " 'Great work for humanity.. thanks krish.. hats off to you for such efforts',\n",
              " 'Can you please guide me on learning NLP with R, I guess most of the libraries mentioned are python based.  I am not sure if these libraries can also be used in R language as well - Please advise. Thanks',\n",
              " 'Sir, will we be able to do text analytics and sentimental analysis after watching this playlist?',\n",
              " 'It  was really helpful. Can u make  videos on Grammer Correction using  Rule based methord, Language Models &  classifiers.\\nits really hard to understand it otherwise',\n",
              " 'I have done several nlp tasks and out of all what I have felt is , unsupervised text categorization , like classifying them based on topic is what I felt difficult especially if problem becomes multilabel',\n",
              " 'I think anyone who is in nlp has eventually followed this kind of roadmap even though the order may be a bit different.',\n",
              " 'Great Sir, Big fan of yours.  Thanks a lot for detailed course.  Hats off.',\n",
              " 'Hi Sir , Can we Directly learn Transformers and BERT ? because u mentioned that the ones before are having some minor problems',\n",
              " 'i come across with all these concepts in my organisation but its hard me to working on question and answering using tensorflow  or pytorch but i am aware of rasa core n rasa nlu where it is easy to generate question and answers. krish why dont you make a video on question and answering using tensorflow',\n",
              " 'This is really a gem',\n",
              " 'Hello Sir I have been following you and have currently completed your Machine Learning playlist. So can you tell me that how much prior knowledge we need to have to start with this course, do we need deep learning, if it is so then please tell me do I need advance deep learning like Boltzmann Machine, Autoencoders, GANs and etc or is it okay to start now.',\n",
              " 'Hi krish, Can you please do video on mutual funds project like sentiment analysis or recommendations. It will helpful alot.',\n",
              " 'I just love how you keep reassuring us in the video that you got us covered from bottom to top. This is super helpful. Thank you',\n",
              " \"He is that teacher which our education system genuinely needs . He's self taught hence that is visible in his lectures. Absolutely amazing !!\",\n",
              " \"And, could you also put up some material on how Hidden Markov Models are used in NLP? have studied them way back in 2011 during my Master's degree in the pre Deep Learning era. But don't have much practical exposure to NLP? And does acoustic model for phonemes recognition come more under speech Recognition? Could you also provide a short description on that?\",\n",
              " 'HI sir ,thanks for sharing your knowledge it really helps me alot sometime, i have a question. \\nif LSTM has problem , why cant we directly use bidirectional LSTM instead of LSTM , can we skip LSTM and directly apply Bidirectional LSTM ?',\n",
              " 'i love how you spelling it, if we could build such a pyramid for other techs, life of the learners would be much easier as it s enough to keep in mind this intuitive/organic scale of complexity.',\n",
              " \"I just can't thank you enough, words would not be able to compensate for your greatness !\",\n",
              " 'At 13:00 you had mentioned about weakness of LSTM. If we take the use case of Statistical Machine Translation and I have 2 sentences in my training set :\\n1) \"I cross the river bank to reach primary school\"\\n2) \"I need to go to the bank to urgently withdraw funds\"\\nBoth are longish sentences. And if after conversion to Vector Representation, the word \"Bank\" has a different meaning in sentence 1) where we need to look at previous word river, while in sentence 2) we need to look ahead (right context).\\nIs this the issue with plain LSTM that Bidirectional LSTM is able to overcome?',\n",
              " 'Have you ever used spacy when it comes to NLP? If yes: Would be lovely if you could cover how you used it...Thanks for your work',\n",
              " 'Great work for humanity.. thanks krish.. hats off to you for such efforts',\n",
              " 'Can you please guide me on learning NLP with R, I guess most of the libraries mentioned are python based.  I am not sure if these libraries can also be used in R language as well - Please advise. Thanks',\n",
              " 'Sir, will we be able to do text analytics and sentimental analysis after watching this playlist?',\n",
              " 'It  was really helpful. Can u make  videos on Grammer Correction using  Rule based methord, Language Models &  classifiers.\\nits really hard to understand it otherwise',\n",
              " 'I have done several nlp tasks and out of all what I have felt is , unsupervised text categorization , like classifying them based on topic is what I felt difficult especially if problem becomes multilabel',\n",
              " 'I think anyone who is in nlp has eventually followed this kind of roadmap even though the order may be a bit different.',\n",
              " 'Great Sir, Big fan of yours.  Thanks a lot for detailed course.  Hats off.',\n",
              " 'Hi Sir , Can we Directly learn Transformers and BERT ? because u mentioned that the ones before are having some minor problems',\n",
              " 'i come across with all these concepts in my organisation but its hard me to working on question and answering using tensorflow  or pytorch but i am aware of rasa core n rasa nlu where it is easy to generate question and answers. krish why dont you make a video on question and answering using tensorflow',\n",
              " 'This is really a gem',\n",
              " 'Hello Sir I have been following you and have currently completed your Machine Learning playlist. So can you tell me that how much prior knowledge we need to have to start with this course, do we need deep learning, if it is so then please tell me do I need advance deep learning like Boltzmann Machine, Autoencoders, GANs and etc or is it okay to start now.',\n",
              " 'Hi krish, Can you please do video on mutual funds project like sentiment analysis or recommendations. It will helpful alot.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Frequency Analysis with count\n",
        "\n",
        "word_frequency_analysis(comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq-NozEGerwm",
        "outputId": "ee4a8c81-c1e2-4feb-da15-fd8c070ce8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words:\n",
            ".: 46\n",
            ",: 44\n",
            "?: 18\n",
            "LSTM: 16\n",
            "need: 12\n",
            "): 10\n",
            "used: 8\n",
            "NLP: 8\n",
            "really: 8\n",
            "question: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "sentiment_analysis(comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-MSmJkjetok",
        "outputId": "fc042292-aed0-497a-89ad-3f5f3c6b3a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sentiment: 0.3786899999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic Modeling\n",
        "\n",
        "topic_modeling(comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZfFFdCEi9Mn",
        "outputId": "de4cde45-e079-4224-f8e3-f055366b5be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words per topic:\n",
            "Topic 1: learning nlp deep used need recognition tell start playlist love\n",
            "Topic 2: lstm really directly thanks sir hi based language libraries bidirectional\n",
            "Topic 3: need lstm bank sentence river look word sentences work thanks\n",
            "Topic 4: thanks thank just hats great able sir course lectures taught\n",
            "Topic 5: question video krish using nlp like rasa tensorflow answering felt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edPVAkaOjWW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}