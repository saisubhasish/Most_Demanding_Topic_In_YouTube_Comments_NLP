{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e328cf",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Take any YouTube videos link and your task is to extract the comments from\n",
    "that videos and store it in a csv file and then you need define what is most\n",
    "demanding topic in that videos comment section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500bda27",
   "metadata": {},
   "source": [
    "### Let's Scrap the comments from a youtube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c135a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8249961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapper code \n",
    "\n",
    "class Data_scraper:\n",
    "    @staticmethod\n",
    "    def scraper(link, path):\n",
    "        \"\"\"\n",
    "        Description: This function scraps comments from youtube videos and stores it to a csv file\n",
    "        =========================================================\n",
    "        Params:\n",
    "        link: link of the video\n",
    "        path: path where file get saved\n",
    "        =========================================================\n",
    "        saves data to a csv file\n",
    "        \"\"\"\n",
    "        driver_path = f\"{path}\\selenium_driver\\chromedriver.exe\"\n",
    "        \n",
    "        try:\n",
    "            # Creating a session and loading the page\n",
    "            driver = webdriver.Chrome(driver_path)\n",
    "            driver.get(link)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            \n",
    "        # Maximizing window\n",
    "        driver.maximize_window()\n",
    "        # wait time\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Scrolling page to view comments\n",
    "        driver.execute_script(\"window.scrollBy(0,500)\",\"\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollBy(0,2000)\",\"\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollBy(0,5000)\",\"\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollBy(0,5000)\",\"\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        \n",
    "        driver.execute_script(\"window.scrollBy(0,2000)\",\"\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        try:\n",
    "            # Beutifulsoup referance\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            \n",
    "            # Find the comment sections\n",
    "            # Getting all the comments with web elements\n",
    "            comment_sections = soup.find_all(\"yt-formatted-string\", class_=\"style-scope ytd-comment-renderer\")\n",
    "        \n",
    "            \n",
    "            # Extract the comments\n",
    "            # Removing the web elements\n",
    "            comments = [comment.text.strip() for comment in comment_sections]\n",
    "\n",
    "            # Saving the list of comments to a pandas dataframe\n",
    "            df = pd.DataFrame(comments)\n",
    "\n",
    "\n",
    "            # Printing each comment\n",
    "            for comment in comments:\n",
    "                print(comment)\n",
    "\n",
    "\n",
    "            print(df)\n",
    "            df.to_csv(f'{path}youtube_comments_scrapped.csv')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        finally:\n",
    "            # Closing driver\n",
    "            driver.quit()\n",
    "            \n",
    "link = input()\n",
    "path = 'D:/FSDS-iNeuron/10.Projects-DS/Most_Demanding_Topic_In_YouTube_Comment_NLP/'\n",
    "\n",
    "Data_scraper.scraper(link, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205bec4",
   "metadata": {},
   "source": [
    "### Lets analyse the data to find the most demanding topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{path}youtube_comments_scrapped.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce74cf",
   "metadata": {},
   "source": [
    "### Storing data to mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8622bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# resetting index\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Converting dataframe to json so that we can dump these record in mongo db\n",
    "json_record = list(json.loads(df.T.to_json()).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6689439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "MONGO_DB_URL = \"mongodb+srv://MongoDB:mongodb123@cluster0.i7o85x8.mongodb.net/?retryWrites=true&w=majority\"\n",
    "mongo_client = pymongo.MongoClient(MONGO_DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_NAME = 'NLP'\n",
    "COLLECTION_NAME = 'youtubeComments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating database\n",
    "mydb = mongo_client[DATABASE_NAME]\n",
    "\n",
    "# Creating collection\n",
    "coll = mydb[COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2df074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert converted json record to mongo db\n",
    "coll.insert_many(json_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa684b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b147e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
